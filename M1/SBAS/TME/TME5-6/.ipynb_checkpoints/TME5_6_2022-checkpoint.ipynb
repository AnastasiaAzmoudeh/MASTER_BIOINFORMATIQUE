{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TzMJ2QtGxpkq"
   },
   "source": [
    "<h1><b>Statistique en Bioinformatique : </b> TME 5 et 6 </h1>\n",
    "<br>\n",
    "L’objectif de ce TME est:\n",
    "<br>\n",
    "<ul>\n",
    "<li> implémenter l'algorithme de Viterbi et l'estimation des paramètres (en utilisant le Viterbi training)\n",
    "pour l'exemple du occasionally dishonest casino.   </li> \n",
    "</ul>\n",
    "<br>\n",
    "<div class=\"alert alert-warning\" role=\"alert\" style=\"margin: 10px\">\n",
    "<p><b>Soumission</b></p>\n",
    "<ul>\n",
    "<li>Renomer le fichier TME5_6.ipynb pour NomEtudiant1_NomEtudiant2.ipynb </li>\n",
    "<li>Soumettre via moodle </li>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UiohDpWCxpkv"
   },
   "source": [
    "Nom etudiant 1 :\n",
    "<br>\n",
    "Nom etudiant 2 :\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i6n5Srpuxpkv"
   },
   "source": [
    "<h3>Introduction</h3>\n",
    "Un casino parfois malhonnête (occasionally dishonest casino) utilise 2 types de pieces : fair et unfair. <br>\n",
    "La matrice de transition entre les états cachés est:<br>\n",
    "${\\cal S}=\\{F,U\\}$ (fair, unfair):\n",
    "$$\n",
    "p = \\left(\n",
    "\\begin{array}{cc}\n",
    "0.99 & 0.01\\\\\n",
    "0.05 & 0.95\n",
    "\\end{array}\n",
    "\\right)\\ ,\n",
    "$$\n",
    "\n",
    "les probabilités d'émission des symboles \n",
    "${\\cal O} = \\{H,T\\}$ (head, tail):\n",
    "\\begin{eqnarray}\n",
    "e_F(H) =  0.5 &\\ \\ \\ \\ &\n",
    "e_F(T) = 0.5 \\nonumber\\\\\n",
    "e_U(H) = 0.9 &\\ \\ \\ \\ &\n",
    "e_U(T) = 0.1 \\nonumber\n",
    "\\end{eqnarray}\n",
    "\n",
    "<br> Et la condition initiale $\\pi^{(0)} = (0.999,0.001)$ (le jeux commence presque toujours avec le pieces juste (fair)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKz1fEeIxpkw"
   },
   "source": [
    "<b>Exercice 1</b>:\n",
    "<u>Simulation</u>: Écrire une fonction qui simule $T$ jets de pièces. \n",
    "La fonction renverra un tableau à deux colonnes correspondant \n",
    "aux valeurs simulées pour les états cachés $X_t$ \n",
    "(type de dés utilisée, “F” ou “U”) et aux symboles observées $Y_t$ \n",
    "(résultat du jet de dés, “H” ou “T”). On simulera une séquence\n",
    "de longueur 2000 qu'on gardera pour les applications ultérieures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "txZ2YCPbxpkx"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#states\n",
    "S = { 0:'F',1 :'U'}\n",
    "\n",
    "#transition probability matrix\n",
    "Pij = np.array([[0.99,0.01], [0.05,0.95]])\n",
    "\n",
    "#emision symbols \n",
    "O = {0:'H', 1: 'T'}\n",
    "\n",
    "#emission probability matrix\n",
    "Ei = np.array([[0.5,0.5], [0.9,0.1]]) #ça aurait dû être Eio\n",
    "\n",
    "# initial Condition\n",
    "pi0=np.array([0.999,0.001])\n",
    "\n",
    "#number of jets\n",
    "T = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5OIqLuzAxpkz"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Fonction qui simule T jets de pieces\n",
    "def jets(T, pi0, Eij, Pij):\n",
    "    \"\"\"\n",
    "      simulation of occasionally dishonest casino\n",
    "      input1 T: number of jets\n",
    "      input2 pi0: initial condition\n",
    "      input3 Eij: emission probability matrix\n",
    "      input4 Pij: transition probability matrix\n",
    "      output1 jetsRes: matrix |2xT| containing simulations\n",
    "    \"\"\"\n",
    "\n",
    "    # Creation du tableau\n",
    "    jetsRes = np.zeros((T,len(pi0)),dtype=int)\n",
    "    \n",
    "   \n",
    "    return jetsRes\n",
    "\n",
    "\n",
    "\n",
    "def printSimulation(resultat):\n",
    "    for i in resultat : \n",
    "        print (S[i[0]], O[i[1]])\n",
    "\n",
    "jetsRes = jets(T, pi0, Ei, Pij)\n",
    "printSimulation(jetsRes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpGzwVdIxpk0"
   },
   "source": [
    "<b>Exercice 2</b>: <u>Algorithme de Viterbi </u>: Écrire une fonction qui permet\n",
    "de déterminer la séquence $(i^\\star_t)_{t=0:T}$ d'états cachés\n",
    "plus probable, ainsi que sa probabilité. Pour tester votre fonction utiliser le résultat de la \n",
    "simulation (2éme colonne) de la question 1. Comparer $(i^\\star_t)_{t=0:T}$ avec\n",
    "les vrais états cachés (1ère colonne de la simulation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GCN_KQ-txpk1"
   },
   "outputs": [],
   "source": [
    "# Algorithme de Viterbi\n",
    "import operator\n",
    "\n",
    "def viterbi(jets, Pij, Ei, pi0, nS, nO, enLog):\n",
    "    \"\"\"\n",
    "    Implement Viterbi algorithm\n",
    "    input1 jets: matrix |2xT| containing simulations\n",
    "    input4 Pij: transition probability matrix\n",
    "    input3 Ei: emission probability matrix\n",
    "    input4 pi0: initial condition\n",
    "    input5 nS: number of states\n",
    "    input6 nO: number of observations\n",
    "    input7 enLog: bool, when True we apply log to avoid underflow\n",
    "    output1 i_star: most problable path\n",
    "    output2 prob: probability associated to the most problable path\n",
    "    \"\"\"\n",
    "    \n",
    "    obs = jets[:,1]\n",
    "    T = len(obs) #Nombre d'observations (longueur des observations)\n",
    "\n",
    "    i_star = np.zeros((T))\n",
    "    prob = 1\n",
    "\n",
    "\n",
    "    return i_star, prob\n",
    "\n",
    "def analyseResultats(jets, estimation):    \n",
    "    \"\"\"\n",
    "    Compare expected and obtained paths\n",
    "    input1 jets: expected path\n",
    "    input2 estimation: obtained path\n",
    "    output1 error: percentage error\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    return error\n",
    "\n",
    "i_est, P_est = viterbi(jetsRes, Pij, Ei, pi0, 2, 2, False)\n",
    "error = analyseResultats(jetsRes, i_est)\n",
    "print('erreur d\\'estimation de viterbi:')\n",
    "print(error,'%')\n",
    "#print('Probabilité estimé:')\n",
    "#print(p_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7NimDYwxpk2"
   },
   "source": [
    "<b>Exercice 3</b>: <u>Estimation des paramètres</u>\n",
    "<br>\n",
    "3.1) Écrire une fonction qu'utilise tous les résultats de la simulation\n",
    "(états et symboles) pour compter les nombres d'occurrence $N_{ij}$ est $M_{iO}$ définis en cours. Estimer $P_{ij}$ est $E_i(O)$. Attention, pour éviter les probabilités à zéro nous allons utiliser les pseudo-count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "msq33Rxjxpk2"
   },
   "outputs": [],
   "source": [
    "# Estimation de Parametres par contage\n",
    "def nombresOccurrence(jets, nS, nO):\n",
    "    \"\"\"\n",
    "    Parameter estimation\n",
    "    input1 jets: matrix |2xT| containing data\n",
    "    input2 nS: number of states\n",
    "    input3 nO: number of observations\n",
    "    output1 Nij: transition probability matrix\n",
    "    output2 Mio: emission probability matrix\n",
    "    output3 pi0: initial condition \n",
    "    \"\"\"\n",
    "\n",
    "    Nij = np.ones((nS,nS)) #pseudo-count = 1\n",
    "    Mio = np.ones((nS,nO))  #pseudo-count = 1\n",
    "    pi0 = np.ones((nS))\n",
    "\n",
    "\n",
    "    \n",
    "    return Nij, Mio, pi0\n",
    "\n",
    "Nij, Mio, pi0 = nombresOccurrence(jetsRes, 2, 2)\n",
    "\n",
    "print('Pij estimé:')\n",
    "print(Nij)\n",
    "print('\\nEio estimé:')\n",
    "print(Mio)\n",
    "print('\\npi0 estimé:')\n",
    "print(pi0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-ZYvpCgxpk3"
   },
   "source": [
    "3.2) <u> Viterbi training </u>: Écrire une fonction qui utilise\n",
    "seulement la séquence $(O_t)_{t=0:T}$ (2emme colonne de la simulation) pour estimer les\n",
    "paramètres $P_{ij}$ est $Ei(O)$. On s’arrêtera quand les différences entre les logVraissamblance est inférieur à 1e-04. Comparer les résultats de 3.1 et de 3.2 (3.2 avec plusieurs restarts,\n",
    "et avec initialisation des paramètres aléatoire).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dhKsGmO2xpk4"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialisation aleatoire de Pij, Eij, pi0\n",
    "def InititRandom(nS, nO):\n",
    "    \"\"\"\n",
    "    randomly initialisations \n",
    "    input1 nS: number of states\n",
    "    input2 nO: number of observations\n",
    "    output1 Pij_init: transition probability matrix\n",
    "    output2 Ei_init: emission probability matrix\n",
    "    output3 pi0_init: initial condition \n",
    "    \"\"\"\n",
    "    random.seed(10)\n",
    "\n",
    "\n",
    "    return Pij_init, Ei_init, pi0_init\n",
    "\n",
    "# Calcule log Vraissamblance\n",
    "def logLikelhihood(Aij,Bij,pi0,jets):\n",
    "    \"\"\"\n",
    "    compute Log Likelihood \n",
    "    input1 Pij: transition probability matrix\n",
    "    input2 Ei: emission probability matrix\n",
    "    input3 pi0: initial condition \n",
    "    input4 jets: matrix |2xT| containing data\n",
    "    \"\"\"\n",
    "    etat_seq = jets[:,0]\n",
    "    obs_seq = jets[:,1]\n",
    "    T = len(obs_seq) #Nombre d'observations (longueur des observations)\n",
    "    lLikelihood = np.log(pi0[etat_seq[0]])\n",
    "\n",
    "\n",
    "    return lLikelihood\n",
    "\n",
    "\n",
    "def Training(jets, nS, nO):\n",
    "    \"\"\"\n",
    "    Viterbi Training\n",
    "    input1 jets: matrix |2xT| containing data\n",
    "    input2 nS: number of states\n",
    "    input3 nO: number of observations\n",
    "    output1 Pij_est: transition probability matrix\n",
    "    output2 Ei_est: emission probability matrix\n",
    "    output3 pi0_est: initial condition \n",
    "    output4 lLikelihood: log Likelihood\n",
    "    \"\"\"\n",
    "    jets_est = np.array(jets)    \n",
    "    Pij_est, Ei_est, pi0_est = InititRandom(nS, nO)\n",
    "    \n",
    "    nIteration = 10000\n",
    "    criterion = 1e-04\n",
    "    lLikelihood = np.zeros((nIteration))\n",
    "   \n",
    "\n",
    "\n",
    "    return Pij_est, Ei_est, pi0_est, lLikelihood\n",
    "    \n",
    "#imprimer les Parametres du Viterbi Training\n",
    "Pij_est, Ei_est, pi0_est, lLikelihood = Training(jetsRes, 2, 2)\n",
    "itCount = len(lLikelihood)\n",
    "print('Le modèle est convergé après '+str(itCount)+' itérations.')\n",
    "print('\\nPij estimée:')\n",
    "print(Pij_est)\n",
    "print('\\nEij estimée:')\n",
    "print(Ei_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6B-AX8Zxpk4"
   },
   "source": [
    "<font color=\"blue\">\n",
    "Remark:\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o7iq2fv5xpk5"
   },
   "source": [
    "3.3) <u>Viterbi training deuxième version</u>. \n",
    "<BR>Écrivez une version de 3.2 qui:\n",
    "- part plusieurs fois (100x) d'une initialisation aléatoire desparamètres de l'HMM,\n",
    "- utilise Viterbi training pour estimer les paramètres,\n",
    "- calcul la log-vraisemblance pour les paramètres estimés,\n",
    "- sauvegarde seulement l'estimation avec la valeur maximale de lalog-vraisemblance.\n",
    "\n",
    "Qu'est-ce que vous observez?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rwS987bpxpk5"
   },
   "outputs": [],
   "source": [
    "# Viterbi Training  deuxiemme version\n",
    "\n",
    "def TrainingV2(jets, nS, nO, nIterat=100):\n",
    "    \"\"\"\n",
    "    Viterbi Training version 2.0\n",
    "    input1 jets: matrix |2xT| containing data\n",
    "    input2 nS: number of states\n",
    "    input3 nO: number of observations\n",
    "    input4 nIterat: number of iterations\n",
    "    output1 Pij_best: best transition probability matrix\n",
    "    output2 Ei_best: best emission probability matrix\n",
    "    output3 pi0_best: best initial condition \n",
    "    output4 lLikelihood_best: best log Likelihood\n",
    "    \"\"\"\n",
    "\n",
    "    Pij_best = []\n",
    "    Ei_best = []\n",
    "    pi0_best = []\n",
    "    lLikelihood_best = -10000\n",
    "    \n",
    "\n",
    "    return Pij_best, Ei_best, pi0_best, lLikelihood_best\n",
    "    \n",
    "\n",
    "# Imprimer les Parametres du Viterbi Training deuxiemme version\n",
    "nIterat = 100\n",
    "Pij_best, Ei_best, pi0_best, lLikelihood_best = TrainingV2(jetsRes, 2, 2, nIterat)\n",
    "\n",
    "print('Meilleur Pij estimée:');\n",
    "print(Pij_best)\n",
    "print('\\nMeilleur Eij estimée:')\n",
    "print(Ei_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4EHk9MKxpk6"
   },
   "source": [
    "<font color=\"blue\">\n",
    "Remark:\n",
    "</font>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TME5_6_2022.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
